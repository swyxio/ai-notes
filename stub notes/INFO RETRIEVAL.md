-   In late 2021, DeepMind proposed [RETRO](https://www.deeplearning.ai/the-batch/large-language-models-shrink/), a model that retrieves passages from the MassiveText dataset and integrates them into its output.
-   AI21 Labs' spring launch of [Jurassic-X](https://www.deeplearning.ai/the-batch/neural-nets-rules-truer-text/)introduced a suite of modules — including a calculator and a system that queries Wikipedia — to fact-check a language model’s answers to math problems, historical facts, and the like.
-   Researchers at Stanford and École Polytechnique Fédérale de Lausanne created [SERAC](https://www.deeplearning.ai/the-batch/update-any-language-model/), a system that updates language models with new information without retraining them. A separate system stores new data and learns to provide output to queries that are relevant to that data.
-   Meta built [Atlas](https://www.deeplearning.ai/the-batch/how-small-language-models-can-perform-specialized-tasks/), a language model that answer questions by retrieving information from a database of documents. Published in August, this approach enabled an 11 billion-parameter Atlas to outperform a 540 billion-parameter PaLM at answering questions.