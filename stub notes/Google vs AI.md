
Pro Google
- Google has insane AI expertise
	- you can try it right now
		- you dot com and neeva https://twitter.com/debarghya_das/status/1611555488776458242?s=46&t=fMwE5qHjZ29z2Apm-SG4mQ
	- Google BERT has been running billions of times a day
		- https://twitter.com/sterlingcrispin/status/1606064294663069696?s=20
		- replaced by MUM in 2021 https://towardsdatascience.com/rip-bert-googles-mum-is-coming-cb3becd9670f 
		- nice diagram https://searchengineland.com/google-mum-update-seo-future-383551
	- Google PaLM (Pathways Language Model) in 2021
		- Pathways will enable us to train a single model to do thousands or millions of tasks.
			- We want a model to have different capabilities that can be called upon as needed, and stitched together to perform new, more complex tasks – a bit closer to the way the mammalian brain generalizes across tasks.
		- Pathways will enable multimodal models that encompass vision, auditory, and language understanding simultaneously. 
			- And of course an AI model needn’t be restricted to these familiar senses; Pathways could handle more abstract forms of data, helping find useful patterns that have eluded human scientists in complex systems such as climate dynamics.
		- Pathways will make models sparse and efficient, only activating the relevant parts of the network for the given task.
			- For example, GShard and Switch Transformer are two of the largest machine learning models we’ve ever created, but because both use sparse activation, they [consume less than 1/10th the energy](https://blog.google/technology/ai/minimizing-carbon-footprint/) that you’d expect of similarly sized dense models — while being as accurate as dense models.
		- PaLM 540B beats GPT3 175B on all categories https://twitter.com/sterlingcrispin/status/1606309065730170880/photo/1
	- Google LaMDA
		- https://blog.google/technology/ai/lamda/
	- agreement from emad
		- https://twitter.com/EMostaque/status/1610609874743738370
- PageRank origins ([tweet](https://twitter.com/mmitchell_ai/status/1605013368560943105?s=20)) 
	- pre pagerank was gpt-retrieval like
	- With PageRank, the fact that websites link to one another could be used to identify which websites were *the most* linked to. The *most linked* sites are the ones people tend to want.
- link vs answer? [ben evans tweet](https://twitter.com/benedictevans/status/1607547804108431362)
	- When you were given a link did you really want an answer? Or did you, in fact, want a link?
	- I would like to see a matrix of query volume versus search ad revenue versus susceptibility to an answer-based response. The respond to "give me a personal injury lawyer" can still have ads even if it's delivered by GPT...







ai is feafure not product
https://twitter.com/yacinemtb/status/1612997351659945986?s=46&t=J346-y4Gcs8C5beSeKYreA


openai was spending 70m  with google cloud before msft investment https://twitter.com/amir/status/1590015635765219336


ai substack clone https://twitter.com/kantrowitz/status/1613924005064462336?s=46&t=d1UrpSjJB1LHnxU9IP9CZQ

hardware
cerebras and graphcore